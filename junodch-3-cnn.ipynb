{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture CNN\n",
    "Train a CNN model then test it on the whole picture and save the result image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as matPlt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from shapely.geometry import Polygon, box\n",
    "\n",
    "import rasterio\n",
    "from rasterio import plot as rastPlt\n",
    "from rasterio.plot import reshape_as_raster\n",
    "from rasterio.merge import merge as rasterMerge\n",
    "from rasterio.mask import mask as rasterMask\n",
    "\n",
    "import junodch_utils_read_img as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "### Fetch data from files\n",
    "Extract the area covered by the image by the satellite (sentinel-2) and extracte all pixels coordinates from the night image (VIIRS).\n",
    "dataCoords will be use to fetch the individual training and testing tile for the machine learning model. dataRadiance contains the light intensity of each tile.\n",
    "dataCoords and dataRadiance have the same size and each entry matches the same tile.\n",
    "the only variable that sould be changed is folderName after fetching the data from 1-fetch-data and merging them in 2-image-merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderName = \"img/Sokoto/\"\n",
    "pathSatellite = folderName + \"Sentinel-2.tif\"\n",
    "pathNight = folderName + \"Night_VIIRS.tif\"\n",
    "pathValidation = folderName + \"Population_GHSL.tif\"\n",
    "\n",
    "aoi = utils.getImgBorder(pathSatellite)\n",
    "\n",
    "# Fetch coords\n",
    "dataCoords, dataRadiance = utils.getTilesCoordsPerimeter(pathNight, area=aoi)\n",
    "print('Tiles:',dataCoords.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Validation\n",
    "Extract the data from GHSL. This is not a perfect metric the validation but will be use to filter some of the data used before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(pathValidation) as f:\n",
    "  sampleTile, _ = rasterMask(f, [Polygon(dataCoords[1000])], crop=True) # arbitrary coordinate for a sample\n",
    "print('Validation shape:',sampleTile.shape) # shape sample\n",
    "\n",
    "print('Process validation...')\n",
    "# If any of the pixels have a settlement then true else false\n",
    "getValid = lambda data : [ int(250/255 < img.max()) for img in data ]\n",
    "resultValid = utils.scanSatellite(pathValidation, dataCoords, getValid, batch=1000, res=sampleTile.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch images\n",
    "Fetch the images to use for the training of the model.\n",
    "As muche as possible we want to be sure that the lightCoords matches tiles containing human settlement pictures and darkCoords matches tiles that are not human settlement.\n",
    "This is why the resultValid is used to mask some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightMask = (dataRadiance>0) & (resultValid == 1)\n",
    "lightCoords = dataCoords[lightMask]\n",
    "darkMask = (dataRadiance==0) & (resultValid == 0)\n",
    "\n",
    "idxDarkData = np.random.choice(np.arange(dataCoords.shape[0])[darkMask], len(lightCoords), replace=False)\n",
    "darkCoords = dataCoords[idxDarkData]\n",
    "\n",
    "trainCoords = np.concatenate((lightCoords, darkCoords))\n",
    "\n",
    "with rasterio.open(pathSatellite) as f:\n",
    "  trainData, _ = utils.coordsToImgsFormated(f, trainCoords, res=64)\n",
    "print(trainData.shape)\n",
    "\n",
    "print('Light Tile:',len(lightCoords))\n",
    "print('dark Tile:',len(darkCoords))\n",
    "print('Total train',trainData.shape)\n",
    "\n",
    "train = trainData\n",
    "validation = np.concatenate((dataRadiance[lightMask], dataRadiance[idxDarkData]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "By default, an unchanged adam optimizer with mse loss function and a relu activation function.  \n",
    "The default layers have : [12, 12, 12] filters\n",
    "The original picture is supposed to be 64x64 pixels and it's reduce into a 8x8 pixels before being flatten.  \n",
    "The final result is one value between 0 and 1.  \n",
    "0 means no human settlement so no light. 1 means light so human settlement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input encoder\n",
    "input_shape = keras.Input(shape=trainData.shape[1:])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(\n",
    "  learning_rate=0.00025,\n",
    "  beta_1=0.9,\n",
    "  beta_2=0.999,\n",
    ")\n",
    "lossFunction = keras.losses.MeanSquaredError() # l2\n",
    "\n",
    "activationFunction = 'relu'\n",
    "\n",
    "earlyStop = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=5)\n",
    "\n",
    "cnn = layers.Conv2D(12,(3,3), 2, padding='same', activation=activationFunction)(input_shape)\n",
    "cnn = layers.Conv2D(12,(3,3), 2, padding='same', activation=activationFunction)(cnn)\n",
    "cnn = layers.Conv2D(12,(3,3), 2, padding='same', activation=activationFunction)(cnn)\n",
    "\n",
    "cnn = layers.Flatten()(cnn)\n",
    "cnn = layers.Dense(1, activation='sigmoid')(cnn)\n",
    "\n",
    "modelCNN = keras.Model(input_shape, cnn)\n",
    "modelCNN.compile(optimizer=optimizer, loss=lossFunction)\n",
    "\n",
    "result = modelCNN.fit(\n",
    "  x=train[::2],\n",
    "  y=validation[::2],\n",
    "  epochs=50,\n",
    "  batch_size=4,\n",
    "  verbose=0,\n",
    "  validation_data=(train[1::2], validation[1::2]),\n",
    "  callbacks=[\n",
    "    TqdmCallback(verbose=1), # Concise display progression\n",
    "    earlyStop,\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matPlt.plot(result.history['loss'][1:], label='Training')\n",
    "matPlt.plot(result.history['val_loss'][1:], label='test')\n",
    "modelCNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse model\n",
    "Calculate all tiles prediction by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Process score...')\n",
    "getScore = lambda data : modelCNN.predict(data, verbose=0).flatten()\n",
    "result = utils.scanSatellite(pathSatellite, dataCoords, getScore, batch=100, res=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the confusion matrix.  \n",
    "**Adjust \"threshold\", this is an arbitrary value to get the best f-score !**\n",
    "\n",
    "The matrix is calculated by comparing the prediction against the GHSL data contained in resultValid.\n",
    "There are no garantees that a false negative or positive are truly false. This will require to be check manually to the file 4-validation-map.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "\n",
    "print('Process confustion matrix...')\n",
    "print('total',len(result))\n",
    "resultTest = (np.asarray(result) > threshold).astype(int)\n",
    "confusionMatrix = confusion_matrix(resultValid, resultTest)\n",
    "print(confusionMatrix)\n",
    "tp = confusionMatrix[1][1]\n",
    "fp = confusionMatrix[0][1]\n",
    "fn = confusionMatrix[1][0]\n",
    "print('f-score:',round(tp / (tp + (fp + fn)/2) * 100, 2),\"%\")\n",
    "print('tp / fp:',round(tp / (tp + fp) * 100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test false positif\")\n",
    "\n",
    "fpTest = dataCoords[(resultTest == 1) & (resultValid == 0)]\n",
    "\n",
    "with rasterio.open(pathSatellite) as f:\n",
    "  dataTest, _ = utils.coordsToImgsFormated(f, fpTest[::100], res=64)\n",
    "\n",
    "utils.displayImgs(dataTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the analysed results\n",
    "On the left is the original image  \n",
    "On the right is the GHSL data  \n",
    "In the center is the resulting image of the model   \n",
    "  \n",
    "Green means tp: This is a human settlement by the model prediction and by GHSL  \n",
    "Blue means tn: This is not a human settlement by the model prediction and by GHSL  \n",
    "Red means fp: The prediction says this is a human settlement but GHSL does not.  \n",
    "Purple means fn: The prediction did not said that was a human settlement but GHSL did.  \n",
    "  \n",
    "In otherword : Green and blue are good. but the rest must be double check in the file 4-validation-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultImg, resultMeta = utils.mapResultOnImg(pathNight, dataCoords, resultTest, resultValid)\n",
    "\n",
    "\n",
    "fig, axs = matPlt.subplots(1,3, dpi=240)\n",
    "with rasterio.open(pathSatellite) as s: utils.displayTiles([s.read()], [s.transform],axs[0])\n",
    "\n",
    "axs[2].set_xlim(axs[0].get_xlim())\n",
    "axs[2].set_ylim(axs[0].get_ylim())\n",
    "\n",
    "with rasterio.open(pathValidation) as p: rastPlt.show(p, ax=axs[2])\n",
    "\n",
    "axs[1].set_xlim(axs[0].get_xlim())\n",
    "axs[1].set_ylim(axs[0].get_ylim())\n",
    "\n",
    "utils.displayTiles([resultImg], [resultMeta], axs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# quick display\n",
    "fig, axs = matPlt.subplots(1,1, dpi=240)\n",
    "utils.displayTiles([resultImg], [resultMeta], axs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde de l'image et du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN.save('model/cnn_64px_12_12_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.copy(resultImg)\n",
    "img = img[0:3,:,:]\n",
    "img = img.transpose([1, 2, 0])\n",
    "with rasterio.open(pathNight) as f:\n",
    "  profile = f.profile\n",
    "img = reshape_as_raster(img)\n",
    "profile.update(count=3)\n",
    "print(profile)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(folderName + 'cnn_64px_12_12_12_V2.tif', 'w', **profile) as f:\n",
    "  f.write(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('BT-Junodch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0877fde3bc78a9e7113f97fea145bff0c5aa8882703ee053f927d63ca7148c47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
